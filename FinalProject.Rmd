---
title: 'Final Project: Guidance Document'
author: "Allen,Julian,Vishnu,Shweta"
date: "Due December 3, 2023"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

#Exploratory question:
  Does public restroom in a given neighborhood affect fecal matter appearance rates in said neighborhood?

#Datasets
We took two data sets. One from ARCGIS San Francisco Poop map A scrapped using esri2sf on the arcgis service layer and bathroom from a wrangled csv using Excel from the San Francisco Government website.

```{r}
rm(list = ls())
library(rvest)
library(lubridate)
library(tidyverse)
library(DataComputing)
#data("SanFranPublicBathrooms.csv")
#We obtained the following from https://data.sfgov.org/City-Infrastructure/Map-of-Public-Water-Fountains/ne3b-nisa
sanfran=read.csv("SanFranPublicBathrooms.csv", header=TRUE)
head(sanfran)
#We found the following github for scraping arcgis layers.
library(remotes)
#install_github("yonghah/esri2sf")
library("esri2sf")

#We found this layer from the pennstate arcgis directory. https://pennstate.maps.arcgis.com/home/item.html?id=f3c1738ad2024439b278baad2ae67a54#data
url <- "https://services.arcgis.com/QPnoxtBFXm6yYtyc/ArcGIS/rest/services/SanFranciscoPoopMap_WFL1/FeatureServer/2"
poop <- esri2sf(url, objectIds = paste(collapse = ","))

head(sanfran)
```

##San Francisco table analysis 
We have important location information for geospatial analysis, but also redundant information such as date loaded, water fountains, and other public water descriptions unused in analysis. We will wrangle it later.

--------------------------------------------------------------------------

```{r}
head(poop)
```
##Poop table analysis
We have a lot of redundant overlapping information such as neighborhood(neighborhood/Neighborhoods)+district(currentPD/PoliceDistrict). We will later define a function that will convert the 5 digit dates of the table into a day between 1-365 for the time period of July 2019 to June 2020.
--------------------------------------------------------------------------


```{r}
library(sf)

wrangled<-
  poop%>%
  mutate(URL=NULL,RequestType=NULL,source=NULL)%>%
  st_drop_geometry(df)

sanfranBR<-
  sanfran%>%
  mutate(uid=NULL, water_fountain=NULL, resource_type=NULL,bottle_filler=NULL,jug_filler=NULL, dog_fountain=NULL, notes=NULL, data_source=NULL,data_as_of=NULL,data_loaded_at=NULL)%>%
  st_drop_geometry(df)
```
#Wrangling
We have two sets of wrangled datasets and later we defined some other subsets of data that still holds geoms. But the basis of our investigation focuses on geographical information and thus we remove information that involves unimportant to our special research question such as water fountains. We remove the geoms from the wrangled dataset to make joins easier since the geoms typically cause the datasets to be perceived as spatial data. However, we will maintain a subset to make the leaflet and compare location information.
--------------------------------------------------------------------------

```{r}
poopAtRestroom<-
  left_join(wrangled, sanfran, by=c('Neighborhood'='analysis_neighborhood'))


x <- wrangled %>%
  select(Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarise(poop_count=n())

y <- sanfran %>%
  select(analysis_neighborhood) %>%
  group_by(analysis_neighborhood) %>%
  summarise(restroom_count=n())

joined_table <- x %>% 
  full_join(y, join_by("Neighborhood" == "analysis_neighborhood"))

joined_table <- joined_table[!is.na(joined_table$Neighborhood),]
joined_table
```
#Join operation
Before the information has been organized, we scale out the entirely of both datasets in one join in order to have a large scale analysis on the possible correlations between the two table. In doing so, we realized bathrooms tend to also differ by whether they are publically or selectively available for use.

--------------------------------------------------------------------------
```{r}

convert_num_to_date = function(date){
  hours_passed = (date %% 39630) * 2
  days = hours_passed %/% 24
  days
}

poop2 <- wrangled %>% mutate(dayReported=convert_num_to_date(wrangled$Date))

supervisor<-
    poop2%>%
    group_by(dayReported,supervisor)%>%
    summarise(count=n())%>%
    pivot_wider(names_from=supervisor, values_from = count, names_glue = "{.value}{supervisor}")
supervisor
```
#Pivot wider
We inspect the trends of supervisors for each day of the year in terms of finding excrement. This will appear in a graph later. 

--------------------------------------------------------------------------
```{r}
street_ending_finder=function(frame, street.col, street.ending){
  pattern <- paste(street.ending, "$", sep="")
  pattern
  Matches <- frame %>%
    filter(grepl(pattern = pattern, street.col, ignore.case = TRUE))
  return(Matches)
}

Matches <- street_ending_finder(poop, poop$Address, "ST")
Matches


for (i in 1:length(joined_table$Neighborhood)){
  case <- joined_table[i,]
  for (j in 1:length(case)){
    variable <- joined_table[i,j]
    if (is.na(variable) ){
      joined_table[i,j] <- 0
    }
  }
}


```
#Finishing touches
  We are clearing up null information within the join table and using regular expression to clear up confusions between the address and street variables in the poop table and the location in San Francisco bathroom tables.

--------------------------------------------------------------------------
#Linear Regression math

We want to explore if a restroom count in a neighborhood has an effect on poop count
in a neighborhood. We will use linear regression to explore this. 

The setup:

$poop\_count_i \sim N(b_0 + b_1 \times restroom\_count_i, \sigma^2)$

The following Hypothesis are used:


\begin{align*}
H_0 &: b_1 = 0 \\
H_a &: b_1 \neq 0
\end{align*}



Approach taught in Stat 415, write a negative log liklihood function.

```{r}

nll.regression = function(theta, poop_count, restroom_count){
  b0 = theta[1]
  b1 = theta[2]
  sigma_sq = theta[3]
  mean = b0 + b1 * restroom_count
  -sum(dnorm(poop_count, mean=mean, sd = sigma_sq, log = TRUE))
}


```

Here we use the optim function which will estimate the MLE for \$ b_0, b_1\$ and $\sigma^2$

```{r}
out = optim(c(0, 0, 1), nll.regression, poop_count=joined_table$poop_count, restroom_count=joined_table$restroom_count, hessian=TRUE)


b1= out$par[2]
I = out$hessian

se = sqrt(diag(solve(I)))

```

Calculate the Wald-Test Statistic and check $P(|Z^*| < 0.05)$

```{r}
Z = (b1 - 0)/se[2]

p_val = 2 * dnorm(-abs(Z))

p_val


```

Since our p_value is less than 0.05, we can reject our null hypothesis and conclude that
the number of restrooms in an area has an effect on the number of poops in that area

--------------------------------------------------------------------------
#VISUALIZATIONS

```{r}
ggplot(supervisor)+
  geom_line(aes(x=dayReported,y=count1,color="District1"))+
  geom_line(aes(x=dayReported,y=count2,color="District2"))+
  geom_line(aes(x=dayReported,y=count3,color="District3"))+
  geom_line(aes(x=dayReported,y=count4,color="District4"))+
  geom_line(aes(x=dayReported,y=count5,color="District5"))+
  geom_line(aes(x=dayReported,y=count6,color="District6"))+
  geom_line(aes(x=dayReported,y=count7,color="District7"))+
  geom_line(aes(x=dayReported,y=count8,color="District8"))+
  geom_line(aes(x=dayReported,y=count9,color="District9"))+
  geom_line(aes(x=dayReported,y=count10,color="District10"))
```
##Line graph
We have the pivot table here as a multi layered graphic about the relation between different districts and their experience with reports throughout the year.
In our initial theory about the trends, we envision areas around tenderloin to have the most. This means we should expect district 6 and 8 to be particularly relevant. However, under deeper analysis, we see that district 9 is higher, but almost no other district reaches the same density. We also found a big dip of information between days 140-200 which puts it squarely in spring 2020 when isolation really started taking place. 

--------------------------------------------------------------------------
```{r}
b0 = out$par[1]
b1 = out$par[2]

reg_func = function(b0, b1, x){
  b0 + b1 * x
}

ggplot(joined_table) + geom_point(aes(x = restroom_count, y = poop_count)) + 
  geom_line(aes(x = restroom_count, y = reg_func(b0, b1, restroom_count), color = "red"))
```
##Linear regression comparing restroom count and poop count.
According to our effect, we find the restrooms in area tends to means more counts. In general, we should expect higher foot traffic in areas with higher bathrooms. However, the case seems to say there are cases where a neighborhood have few public bathrooms and they tend to have regular cases. However, it goes expontentially higher for places with a decent amount, but drops back down for when the instrastructure has a lot of bathrooms. Bathrooms have an effect that isn't strictly linear, but rather seems to be an indictation of the nature of San Francisco infrastructure.

--------------------------------------------------------------------------
```{r}
wrangledA <- data.frame(Neighborhood = sample(c("A", "B", "C", "D"), 1000, replace = TRUE))

sanfranA <- data.frame(analysis_neighborhood = sample(c("A", "B", "C", "D"), 1000, replace = TRUE))

x <- wrangledA %>%
  select(Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarise(poop_count = n())

y <- sanfranA %>%
  select(analysis_neighborhood) %>%
  group_by(analysis_neighborhood) %>%
  summarise(restroom_count = n())

joined_table <- x %>% full_join(y, by = c("Neighborhood" = "analysis_neighborhood"))


ggplot(joined_table, aes(x = Neighborhood)) +
  geom_bar(aes(y = poop_count, fill = "Poop Count"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = restroom_count, fill = "Restroom Count"), stat = "identity", position = "dodge") +
  labs(title = "Poop and Restroom Counts in San Francisco Neighborhoods",
       x = "Neighborhood",
       y = "Count",
       fill = "Legend") +
  scale_fill_manual(values = c("Poop Count" = "#FF5733", "Restroom Count" = "#33B5E5")) +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank()) +
  scale_y_continuous(expand = expansion(mult = c(0.1, 0.2)))
```
#Sampling and Bucketing
In order to view neighborhoods, we take random samples of how in general, excrement geographically happen near bathrooms at least relatively. However, it does appear that you will find neighborhoods where this is not the case. This seems to build towards the expectation that we can view this is part of city planning and the council's correction towards these data points.

--------------------------------------------------------------------------
```{r}
#devtools::install_github("rstudio/leaflet")
library(leaflet)
library(htmltools)
knitr::opts_chunk$set(echo = TRUE)

html <- list()
  html <- c(html, 
            list(h3(paste0("Map")),
                leaflet() %>%
  addTiles() %>%
   addMarkers(clusterOptions = markerClusterOptions(), data=poop) %>%
   addCircleMarkers(radius=2, color="red", data=sanfran) %>%
   setView(-122.44, 37.76849, zoom=12)%>% 
   addLegend(position = "bottomleft",
                         colors = c("red", "orange"),
                         labels = c("Public_Restrooms", "Poop Clusters are numbered"),
                         title = "Marker Categories")
                 )
            )

tagList(html)
```
#Leaflet
In order to represent the data, we recreated a similar graph to the arcgis map, but as clusters and included bathrooms because their raw data is to produce purely points that can be used in rasanter functions whereas we want to use poops rather as a general point towards their relations to bathrooms.
Calling back to our previous conclusions, we find that areas with very few bathrooms tend to have regular occurrences along the intersection of bathrooms. Furthermore we can find a large set of excrement adjacent to bathrooms in high density areas.


#Link to GITHUB Repo
https://github.com/AG2025DS/Stat184FinalProject
